# Next Sprint Builder Prompt — Debate → Judge → Publish (Phase 2)

**Prepared for:** Kyle  
**Date:** 2025-09-28 21:06 UTC  
**Repo:** `C:\\Users\\kylem\\openai-agents-workflows-2025.09.28-v1`

---

## Context (what exists now)

You have a working **DJP pipeline** in `src/` (debate, judge, publish), an allow‑listed **verbatim publish** rule, guardrails for long quotes, and a CLI that writes timestamped Markdown logs. Keep this as the canonical baseline; we’re extending it without breaking behavior.

---

## Mission (what to build next, in order)

You’ll implement **six** enhancements with tests and logs:
1) **Byte‑exact publish tests** and deterministic **tie‑breakers**.  
2) **Run artifacts (JSON)** for analytics and a tiny loader API.  
3) **“Grounded mode (lite)”**: require citations and penalize ungrounded drafts.  
4) **Policy packs** for the publish allow‑list (switch via CLI).  
5) **Cost/latency caps + fast‑path short‑circuit**.  
6) **Night Shift integration:** presets + task generator and queue tests.

Each enhancement has file targets, acceptance tests, and commands.

---

## Dependencies

Run once (project root):
```powershell
pip install -U pytest rich ujson
```
(Optional UI, for later): `pip install streamlit`

---

## 1) Tests: verbatim publish & tie‑breakers

**Create:** `tests/test_publish_and_ties.py`

**Requirements**
- Add **byte‑for‑byte** equality assertions between the published text and the selected draft.
- Add deterministic tie‑breakers when two drafts have the same total score:
  1. Prefer a provider in `ALLOWED_PUBLISH_MODELS`.
  2. If still tied, prefer the higher **Factual Support** sub‑score (you’ll add sub‑scores in §3).
  3. If still tied, use a stable provider order: `openai/*`, then others.

**Edits**
- `src/publish.py`: implement tie‑breaker logic (pure code; no model calls).
- `src/judge.py`: ensure the judge outputs sub‑scores so tie‑breakers have data.

**Acceptance**
```powershell
pytest -q tests/test_publish_and_ties.py
```

---

## 2) Run artifacts (JSON) + loader

**Create:** `src/artifacts.py`

**Requirements**
- After each run, write `runs/{timestamp}.json` with: task, parameters, all drafts (provider, answer, evidence, confidence, safety_flags), judge scores/sub‑scores, selected provider, publish status.
- Add `load_artifact(path) -> dict` to rehydrate a run.
- Update `src/run_workflow.py` to save both the Markdown log **and** the JSON artifact.

**Acceptance**
- Run the CLI once; confirm a JSON appears in `runs/`.
- Use a tiny script in `python -c "import json;print(json.load(open('runs/<file>.json'))['publish']['provider'])"`

---

## 3) Grounded mode (lite): require citations

**Edits**
- `src/debate.py`: update debater instructions to **always** include 2–5 citations when `require_citations=True`.
- `src/judge.py`: extend the rubric to include **sub‑scores** and a **citation check**:
  - `task_fit (0–4)`, `support (0–4)`, `clarity (0–2)` → **total 0–10**.
  - If `require_citations=True`, apply a penalty when fewer than N citations are present.
  - Return sub‑scores in each `ScoredDraft`.
- `src/schemas.py`: add optional `subscores` field to `ScoredDraft` as `{"task_fit":int,"support":int,"clarity":int}`.
- `src/run_workflow.py`: add `--require_citations` (int, default 0).

**Acceptance**
```powershell
python -m src.run_workflow --task "Summarize the hydrogen aviation market in 150 words with 3 sources." --require_citations 3 --trace_name grounded-lite
# Expect: drafts missing citations receive lower 'support' or penalties; winner shows >=3 citations.
```

---

## 4) Policy packs for allow‑list switching

**Create:** `policies/openai_only.json`, `policies/openai_preferred.json`, `policies/none.json`

Examples:
```json
// policies/openai_only.json
{ "ALLOWED_PUBLISH_MODELS": ["openai/gpt-4.1","openai/gpt-4o","openai/gpt-4o-mini"] }
```
```json
// policies/openai_preferred.json
{ "ALLOWED_PUBLISH_MODELS": ["openai/gpt-4.1","openai/gpt-4o","openai/gpt-4o-mini","google/gemini-1.5-pro","anthropic/claude-3-5-sonnet-20240620"] }
```
```json
// policies/none.json
{ "ALLOWED_PUBLISH_MODELS": [] }
```

**Edits**
- `src/config.py`: add `load_policy(path_or_name) -> list[str]`.
- `src/run_workflow.py`: add `--policy` (default: `openai_only`), mapping names to files in `/policies`.
- Publisher should **still** enforce verbatim rule unchanged.

**Acceptance**
```powershell
python -m src.run_workflow --task "150-word VTOL brief with 3 sources." --policy openai_only
python -m src.run_workflow --task "150-word VTOL brief with 3 sources." --policy none
# Expect: first run publishes; second run downgrades to "advisory_only".
```

---

## 5) Cost/latency caps + fast‑path short‑circuit

**Edits**
- `src/debate.py`: accept `max_debaters` and `timeout_s`; allow skipping non‑OpenAI debaters when `--fastpath` is set.
- `src/judge.py`: if the top‐ranked score margin ≥ `MARGIN_THRESHOLD` (e.g., 2 points), **skip** any second pass.
- `src/run_workflow.py`: add flags `--fastpath` (bool), `--max_debaters` (int), `--timeout_s` (int), `--margin_threshold` (int).

**Acceptance**
```powershell
python -m src.run_workflow --task "150-word brief..." --fastpath --max_debaters 2 --margin_threshold 2 --trace_name fastpath
# Expect: only two debaters run; if judge margin >= 2, no extra work is triggered.
```

---

## 6) Night Shift integration — presets & queue tests

**Create:** `presets/` with 3 templates, e.g.:
- `market-brief-200w.task.md`
- `competitor-snapshot-200w.task.md`
- `exec-memo-250w.task.md`

Add `scripts/make_tasks.py` to copy a preset N times with different `TRACE_NAME`s and timestamps.

**Acceptance**
```powershell
python scripts/make_tasks.py --preset presets/market-brief-200w.task.md --count 3 --out tasks
python nightshift_runner.py --repo . --tasks-dir .\tasks --interval 0 --oneshot
dir *.md  # confirm new run logs
```

---

## Commands (copy/paste)

**Windows PowerShell**
```powershell
cd C:\\Users\\kylem\\openai-agents-workflows-2025.09.28-v1

# 0) deps
pip install -U pytest rich ujson

# 1) tests for verbatim & ties
pytest -q tests/test_publish_and_ties.py

# 2) grounded mode
python -m src.run_workflow --task "Summarize hydrogen aviation in 150 words with 3 sources." --require_citations 3 --trace_name grounded-lite

# 3) policy packs demo
python -m src.run_workflow --task "150-word VTOL brief with 3 sources." --policy openai_only
python -m src.run_workflow --task "150-word VTOL brief with 3 sources." --policy none

# 4) fast path
python -m src.run_workflow --task "150-word brief..." --fastpath --max_debaters 2 --margin_threshold 2 --trace_name fastpath

# 5) night shift queue
python scripts/make_tasks.py --preset presets/market-brief-200w.task.md --count 3 --out tasks
python nightshift_runner.py --repo . --tasks-dir .\tasks --interval 0 --oneshot
```

**macOS/Linux**
```bash
cd ~/openai-agents-workflows-2025.09.28-v1
pip install -U pytest rich ujson
pytest -q tests/test_publish_and_ties.py
python -m src.run_workflow --task "Summarize hydrogen aviation in 150 words with 3 sources." --require_citations 3 --trace_name grounded-lite
python -m src.run_workflow --task "150-word VTOL brief with 3 sources." --policy openai_only
python -m src.run_workflow --task "150-word VTOL brief with 3 sources." --policy none
python -m src.run_workflow --task "150-word brief..." --fastpath --max_debaters 2 --margin_threshold 2 --trace_name fastpath
python scripts/make_tasks.py --preset presets/market-brief-200w.task.md --count 3 --out tasks
python nightshift_runner.py --repo . --tasks-dir ./tasks --interval 0 --oneshot
```

---

## Output checklist (what you’ll see)

- ✅ New tests pass.  
- ✅ `runs/` contains JSON artifacts per run.  
- ✅ Grounded mode behavior visible in judge sub‑scores and winner selection.  
- ✅ Policy switch flips publish → advisory when allow‑list is empty.  
- ✅ Fast‑path reduces calls with small quality sacrifice (document scores).  
- ✅ Night Shift queue processes presets and logs runs.

---

## Hand‑off line for your coding agent

When it asks “what should I do next,” say:

> “Open **2025.09.28-2106-NEXT-SPRINT-BUILDER-PROMPT.md**. Implement each numbered enhancement in order with tests and logs. Keep the publish step verbatim from allow‑listed providers. Report diffs and create a new timestamped run log when done.”
