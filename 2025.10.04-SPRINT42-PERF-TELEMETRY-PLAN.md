# Sprint 42: Perf & Telemetry Groundwork ‚Äî PLAN

**Date**: 2025-10-04
**Branch**: `sprint/42-perf-telemetry`
**Version Target**: v1.0.2-dev
**Prior Sprint**: Sprint 41 (DX + Hardening) ‚Üí v1.0.1 GA

---

## Mission

Lay groundwork for performance monitoring and telemetry without changing runtime behavior. Focus on instrumentation stubs, test duration tracking, and documentation for future performance optimization work.

**Critical Constraints:**
- ‚úÖ Non-breaking changes only
- ‚úÖ No runtime behavior changes
- ‚úÖ Telemetry must be opt-in with noop default
- ‚úÖ All tests must remain passing

---

## Scope

### 1) Test Duration Tracking
**Goal**: Identify slow tests and track performance regressions over time

**Deliverables:**
- Update `.github/workflows/nightly.yml` to include `--durations=25` flag
- Create `scripts/report_slowest.py` to analyze and report test timings
- Document test performance baseline in new section of DEVELOPMENT.md

**Acceptance Criteria:**
- Nightly workflow outputs top 25 slowest tests
- Script can parse pytest output and generate markdown reports
- Baseline documented for future comparison

### 2) Telemetry Toggle Infrastructure
**Goal**: Prepare for future telemetry integration without changing current behavior

**Deliverables:**
- Add `TELEMETRY_ENABLED` environment variable (default: `false`)
- Create `src/telemetry/__init__.py` with noop stub functions:
  - `track_event(event_name, properties=None)` ‚Üí noop
  - `track_metric(metric_name, value, tags=None)` ‚Üí noop
  - `track_timing(operation_name, duration_ms, tags=None)` ‚Üí noop
- Add telemetry configuration to `configs/.env.example`
- Document telemetry toggle in OPERATIONS.md

**Acceptance Criteria:**
- Telemetry module importable and callable
- All functions are noop when `TELEMETRY_ENABLED=false` (default)
- No dependencies added for telemetry backends yet
- Zero runtime overhead when disabled

### 3) Performance Documentation
**Goal**: Guide future optimization work and establish best practices

**Deliverables:**
- Add "Performance Considerations" section to CONTRIBUTING.md
  - Guidelines for writing performant code
  - When to use `@pytest.mark.slow`
  - Profiling tools and techniques
- Update OPERATIONS.md with "Performance Monitoring" section
  - How to enable telemetry (when implemented)
  - Performance baseline metrics
  - Alerting thresholds (placeholder)

**Acceptance Criteria:**
- Clear guidelines for contributors on performance
- Operations team has performance monitoring playbook
- Documentation links to test duration reports

### 4) Config Validation Enhancement
**Goal**: Add validation for new telemetry settings

**Deliverables:**
- Update `src/config/validate.py` to check `TELEMETRY_ENABLED` boolean
- Add test case in `tests/test_config_validation.py` for telemetry vars
- Ensure validation passes with default values

**Acceptance Criteria:**
- Config validation accepts `TELEMETRY_ENABLED=true/false`
- Invalid values rejected with clear error message
- All existing tests still pass

---

## Files to Create

### New Files
- `src/telemetry/__init__.py` ‚Äî Noop telemetry stubs
- `src/telemetry/noop.py` ‚Äî Noop backend implementation
- `scripts/report_slowest.py` ‚Äî Test duration analysis tool
- `tests/test_telemetry.py` ‚Äî Unit tests for telemetry stubs

### Modified Files
- `.github/workflows/nightly.yml` ‚Äî Add --durations=25 flag
- `configs/.env.example` ‚Äî Add TELEMETRY_ENABLED variable
- `src/config/validate.py` ‚Äî Validate telemetry config
- `tests/test_config_validation.py` ‚Äî Test telemetry validation
- `CONTRIBUTING.md` ‚Äî Add performance guidelines
- `docs/OPERATIONS.md` ‚Äî Add performance monitoring section
- `DEVELOPMENT.md` ‚Äî Document test performance baseline
- `CHANGELOG.md` ‚Äî Add Unreleased section for v1.0.2

---

## Implementation Plan

### Phase 1: Infrastructure (30 min)
1. Create `sprint/42-perf-telemetry` branch
2. Bump version to 1.0.2-dev in `src/version.py` and `pyproject.toml`
3. Add Unreleased section to CHANGELOG.md

### Phase 2: Telemetry Stubs (45 min)
4. Create `src/telemetry/__init__.py` with noop functions
5. Create `src/telemetry/noop.py` backend
6. Add `TELEMETRY_ENABLED` to `.env.example`
7. Update `src/config/validate.py` for telemetry validation
8. Write unit tests in `tests/test_telemetry.py`

### Phase 3: Test Duration Tracking (30 min)
9. Update `.github/workflows/nightly.yml` with --durations flag
10. Create `scripts/report_slowest.py` analysis script
11. Document baseline in DEVELOPMENT.md

### Phase 4: Documentation (30 min)
12. Add performance section to CONTRIBUTING.md
13. Update OPERATIONS.md with monitoring playbook
14. Update CHANGELOG.md with all changes

### Phase 5: Validation (15 min)
15. Run full test suite locally
16. Verify config validation
17. Check CI passes
18. Open PR to main

**Estimated Total**: 2.5 hours

---

## Testing Strategy

### Unit Tests
- Telemetry functions callable and return None
- Config validation accepts valid telemetry values
- Config validation rejects invalid telemetry values

### Integration Tests
- Telemetry disabled by default
- Telemetry can be enabled via environment variable
- No runtime overhead when disabled

### Smoke Tests
- All existing e2e tests still pass
- No new test failures introduced

---

## Success Criteria

- ‚úÖ All tests passing (unit + integration + e2e)
- ‚úÖ Zero runtime behavior changes
- ‚úÖ Telemetry stubs callable but noop
- ‚úÖ Test duration tracking functional in nightly CI
- ‚úÖ Documentation complete and accurate
- ‚úÖ Non-breaking changes only
- ‚úÖ CI passes on all supported Python versions

---

## Future Work (Out of Scope)

Not included in Sprint 42:
- ‚ùå Actual telemetry backend integration (e.g., OpenTelemetry)
- ‚ùå Performance optimization of slow tests
- ‚ùå Real-time performance monitoring dashboard
- ‚ùå Alerting on performance regressions
- ‚ùå Distributed tracing implementation

These will be addressed in future sprints after v1.0.2.

---

## Risk Assessment

**Low Risk Sprint:**
- Only adding stubs and documentation
- No dependencies added
- No runtime behavior changes
- Easy to review and validate

**Potential Issues:**
- Test duration tracking may reveal unexpectedly slow tests
  - *Mitigation*: Document baseline, don't optimize yet
- Noop overhead concerns
  - *Mitigation*: Function calls are extremely cheap, validate with profiling if needed

---

## Dependencies

### Required
- Sprint 41 (v1.0.1 GA) must be shipped ‚úÖ
- All v1.0.1 tests passing ‚úÖ
- Clean main branch ‚úÖ

### Blockers
None identified.

---

## Rollout Plan

1. Complete Sprint 42 implementation
2. Open PR: `sprint/42-perf-telemetry` ‚Üí `main`
3. CI validation (all tests pass)
4. Code review
5. Merge to main
6. Tag v1.0.2-dev (development release, not GA)
7. Monitor for issues
8. Plan Sprint 43 (TBD: actual telemetry integration or other priorities)

---

## Notes

- This sprint establishes **infrastructure only** ‚Äî no behavior changes
- Telemetry is **opt-in** and **disabled by default**
- Test duration tracking helps prioritize future optimization work
- Documentation ensures contributors understand performance expectations
- Sprint 43+ will implement actual telemetry backends if needed

---

**Status**: üìã PLANNING COMPLETE ‚Äî Ready to Start
**Next Step**: Create branch and begin Phase 1 (Infrastructure)
