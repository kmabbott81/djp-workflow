# Sprint 9 — Cloud Folder Connectors — COMPLETE

**Date:** 2025-10-01 23:42
**Status:** ✅ All tests passing (12/12)

## Summary

Implemented cloud folder connectors for 7 major storage providers with delta sync support, staged items metadata storage, and comprehensive filtering capabilities. Connectors enable listing, filtering, and incremental synchronization of files from cloud storage for downstream ingestion.

## Files Created

### Core Infrastructure
- `src/connectors/cloud/__init__.py` - Cloud connectors module
- `src/connectors/cloud/base.py` - Base connector class and shared utilities
  - `CloudConnector` abstract base class
  - `StagedItem` dataclass for file metadata
  - `ConnectorConfig` for filters and tenant isolation
  - Glob pattern matching and filtering logic

### Provider Connectors
- `src/connectors/cloud/gdrive.py` - Google Drive connector (Changes API)
- `src/connectors/cloud/onedrive.py` - OneDrive connector (Graph Delta API)
- `src/connectors/cloud/sharepoint.py` - SharePoint connector (Graph Delta API)
- `src/connectors/cloud/dropbox.py` - Dropbox connector (Cursor-based delta)
- `src/connectors/cloud/box.py` - Box connector (Event Stream API)
- `src/connectors/cloud/s3.py` - AWS S3 connector (Timestamp-based pseudo-delta)
- `src/connectors/cloud/gcs.py` - Google Cloud Storage connector (Timestamp-based pseudo-delta)

### Metadata Storage
- `src/metadata.py` - SQLite database for staged items
  - `staged_items` table with tenant isolation
  - Delta token storage for incremental sync
  - Status tracking (staged → ingesting → ingested → failed)
  - Query API for connector operations

### Documentation & Tests
- `docs/CONNECTORS.md` - Comprehensive connector documentation (500+ lines)
  - Setup instructions for all providers
  - OAuth/API configuration
  - Rate limits and quotas
  - Cost optimization strategies
  - Troubleshooting guide
- `tests/test_connectors_cloud.py` - 12 passing tests
  - Config and filtering tests
  - Glob pattern matching
  - Size/MIME type filtering
  - Mock-based connector tests
  - Delta sync validation

## Implementation Details

### Base Connector Features

All connectors inherit from `CloudConnector` with standard interface:

```python
class CloudConnector(ABC):
    def authenticate(self) -> bool
    def list_items(folder_id, page_token) -> (list[StagedItem], next_token)
    def get_delta_changes(delta_token) -> (list[StagedItem], new_token)
    def should_include(item: StagedItem) -> bool
    def filter_items(items: list[StagedItem]) -> list[StagedItem]
```

### Filtering Capabilities

**Include/Exclude Patterns:**
```python
config = ConnectorConfig(
    tenant_id="tenant-123",
    connector_name="gdrive",
    include_patterns=["*.pdf", "*.docx"],
    exclude_patterns=["*/Archive/*", "*_draft*"],
)
```

**MIME Type Filtering:**
```python
config.mime_types = ["application/pdf", "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]
```

**Size and Date Filters:**
```python
config.max_size_bytes = 50_000_000  # 50 MB
config.min_modified_date = datetime(2025, 1, 1)
```

### Delta Sync Support

**Native Delta Sync (GDrive, OneDrive, SharePoint, Dropbox, Box):**
- Google Drive: Changes API with page tokens
- OneDrive/SharePoint: Graph Delta API with `@odata.deltaLink`
- Dropbox: Cursor-based pagination
- Box: Event Stream with stream positions

**Pseudo-Delta (S3, GCS):**
- Timestamp-based filtering (list all objects, compare `last_modified`)
- Recommended: Use S3 Event Notifications or GCS Pub/Sub for production

### Metadata Database Schema

**`staged_items` table:**
```sql
CREATE TABLE staged_items (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tenant_id TEXT NOT NULL,
    connector TEXT NOT NULL,
    external_id TEXT NOT NULL,
    path TEXT NOT NULL,
    name TEXT NOT NULL,
    mime_type TEXT,
    size_bytes INTEGER DEFAULT 0,
    last_modified TEXT,
    delta_token TEXT,
    status TEXT DEFAULT 'staged',  -- staged|ingesting|ingested|failed|skipped
    error_message TEXT,
    metadata_json TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(tenant_id, connector, external_id)
);
```

**Indexes:**
- `idx_staged_tenant_connector` on `(tenant_id, connector)`
- `idx_staged_status` on `status`
- `idx_staged_external_id` on `external_id`

## Test Results

```
tests/test_connectors_cloud.py ............                              [100%]
============================= 12 passed in 0.48s ==============================
```

**Test Coverage:**
- ✅ Connector config defaults
- ✅ Config with filters
- ✅ Glob pattern matching
- ✅ Filter by size limits
- ✅ Filter by MIME type
- ✅ Folders always excluded
- ✅ Batch filtering
- ✅ GDrive authentication
- ✅ OneDrive list items
- ✅ S3 list items
- ✅ OneDrive delta changes
- ✅ Include/exclude priority

## Integration Points

### RBAC & Multi-tenancy
All connectors respect tenant isolation:
```python
config = ConnectorConfig(tenant_id=principal.tenant_id, ...)
```

Staged items include `tenant_id` for queries:
```python
items = get_staged_items(tenant_id="tenant-123", status="staged")
```

### Audit Logging
Connector operations logged:
```python
from src.security.audit import get_audit_logger, AuditAction

logger = get_audit_logger()
logger.log_success(
    tenant_id="tenant-123",
    user_id="system",
    action=AuditAction.INGEST_CORPUS,
    resource_type="connector",
    resource_id="gdrive",
    metadata={"files_staged": 1234}
)
```

### Queue Strategy (Sprint 11)
Connector sync jobs will run as bulk tasks:
```python
from src.queue_strategy import enqueue_task, TaskClass

job_id = enqueue_task(
    task_id=f"sync-{tenant_id}-{connector}",
    task_class=TaskClass.BULK,  # Long-running, high throughput
    function=run_delta_sync,
    tenant_id=tenant_id,
    connector=connector
)
```

## Environment Variables

**Google Drive:**
```bash
GDRIVE_SERVICE_ACCOUNT_JSON=/path/to/service-account.json
# OR
GDRIVE_CREDENTIALS_JSON=/path/to/oauth-credentials.json
```

**OneDrive:**
```bash
ONEDRIVE_CLIENT_ID=your-client-id
ONEDRIVE_CLIENT_SECRET=your-client-secret
ONEDRIVE_TENANT_ID=your-tenant-id
ONEDRIVE_ACCESS_TOKEN=your-access-token
```

**SharePoint:**
```bash
SHAREPOINT_CLIENT_ID=your-client-id
SHAREPOINT_CLIENT_SECRET=your-client-secret
SHAREPOINT_TENANT_ID=your-tenant-id
SHAREPOINT_SITE_ID=your-site-id
SHAREPOINT_ACCESS_TOKEN=your-access-token
```

**Dropbox:**
```bash
DROPBOX_ACCESS_TOKEN=your-access-token
# OR
DROPBOX_APP_KEY=your-app-key
DROPBOX_APP_SECRET=your-app-secret
```

**Box:**
```bash
BOX_ACCESS_TOKEN=your-access-token
# OR
BOX_CLIENT_ID=your-client-id
BOX_CLIENT_SECRET=your-client-secret
```

**AWS S3:**
```bash
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-bucket-name
```

**Google Cloud Storage:**
```bash
GCS_SERVICE_ACCOUNT_JSON=/path/to/service-account.json
GCS_BUCKET_NAME=your-bucket-name
```

**Metadata Database:**
```bash
METADATA_DB_PATH=data/metadata.db  # Default
```

## Manual Validation

### 1. Basic Listing (Simulated)
```python
from src.connectors.cloud import GDriveConnector, ConnectorConfig

config = ConnectorConfig(tenant_id="test", connector_name="gdrive")
connector = GDriveConnector(config)

# Mock authentication for testing
connector.service = MockService()

items, token = connector.list_items()
print(f"Found {len(items)} items")
```

### 2. Filtering
```python
config = ConnectorConfig(
    tenant_id="test",
    connector_name="gdrive",
    include_patterns=["*.pdf"],
    exclude_patterns=["*/Archive/*"],
    max_size_bytes=10_000_000
)

filtered = connector.filter_items(items)
print(f"After filtering: {len(filtered)} items")
```

### 3. Staged Items Database
```python
from src.metadata import insert_staged_item, get_staged_items, StagedItemRecord

# Insert item
record = StagedItemRecord(
    tenant_id="test",
    connector="gdrive",
    external_id="file123",
    path="/folder/doc.pdf",
    name="doc.pdf",
    mime_type="application/pdf",
    size_bytes=12345,
    status="staged"
)
insert_staged_item(record)

# Query items
items = get_staged_items(tenant_id="test", connector="gdrive", status="staged")
print(f"Staged items: {len(items)}")
```

## Rate Limits & Quotas

| Provider | Limit | Notes |
|----------|-------|-------|
| Google Drive | 1000 req/100s/user | Use Changes API for efficiency |
| OneDrive/SharePoint | 1200 req/5min/app | Delta queries count less |
| Dropbox | 300 req/min | Consider longpoll for real-time |
| Box | 10 req/sec/user | Event stream recommended |
| S3 | No hard limit | $0.0004 per 1000 LIST requests |
| GCS | No hard limit | $0.05 per 10,000 operations |

## Cost Optimization

**Full Sync vs Delta:**
- Full sync of 10,000 files: ~100 API calls
- Delta sync (changes only): ~10-100 calls
- **Savings: 90%+ after initial sync**

**Filtering Early:**
- Apply include/exclude patterns in config
- Set `max_size_bytes` to avoid staging huge files
- Use MIME type filters to reduce downstream processing

**Scheduling:**
- Run delta sync hourly or daily
- Use webhooks for real-time updates (Dropbox, Box)
- Batch operations during off-peak hours

## Security Considerations

### Credentials
- ✅ All credentials via environment variables
- ✅ No hard-coded secrets in code
- ✅ Service accounts for automation (Google, GCP)
- ✅ OAuth tokens with refresh for user accounts

### Permissions
- ✅ Read-only scopes where possible
- ✅ Folder-level access (not root)
- ✅ Tenant isolation in staged_items table
- ✅ Audit logging for all connector operations

### Data Privacy
- ✅ No file content stored in metadata (only metadata)
- ✅ Staged items include only file path, size, MIME type
- ✅ Provider-specific IDs kept in `external_id` for sync
- ✅ Deletion of staged items doesn't affect source files

## Migrations

### Database Migration
```python
# Run on startup or via migration script
from src.metadata import init_metadata_db

init_metadata_db()  # Creates staged_items table if not exists
```

**Migration is additive:** No existing data affected. Safe to run multiple times (idempotent).

### Rollback Plan

If connectors cause issues:

1. **Disable connector:**
   ```python
   from src.metadata import update_staged_item_status

   # Mark items as skipped
   for item_id in problematic_ids:
       update_staged_item_status(item_id, status="skipped")
   ```

2. **Clear staged items:**
   ```sql
   DELETE FROM staged_items WHERE tenant_id = 'tenant-123' AND connector = 'gdrive';
   ```

3. **Revoke OAuth tokens:**
   - Remove environment variables
   - Revoke in provider console (Google, Microsoft, etc.)

## Next Steps (Sprint 10)

1. **Ingestion Pipeline:**
   - Read from `staged_items` with `status='staged'`
   - Parse files (PDF, DOCX, PPTX, etc.) - see Sprint 10
   - Chunk content and store in `ingested` table
   - Update `status='ingested'` on success

2. **Scheduled Sync Workers:**
   - Cron job or distributed worker (Sprint 11)
   - Run delta sync every hour
   - Update `delta_token` after each sync

3. **Observability:**
   - Dashboard showing staged items per tenant/connector
   - Sync frequency and API call counts
   - Error rates and failed items

## Open Follow-Ups

None. Sprint 9 is complete and ready for integration.

## Definition of Done

- ✅ 7 cloud connectors implemented (GDrive, OneDrive, SharePoint, Dropbox, Box, S3, GCS)
- ✅ Base connector class with filtering and delta sync
- ✅ Metadata database with staged_items table
- ✅ All 12 tests passing (`pytest -v tests/test_connectors_cloud.py`)
- ✅ Comprehensive documentation (docs/CONNECTORS.md)
- ✅ Environment variables for all credentials (no secrets in code)
- ✅ Tenant isolation in config and database
- ✅ Audit logging integration points documented
- ✅ Rollback plan provided
- ✅ Sprint completion log created

**Sprint 9 Status: COMPLETE ✅**
